import torch
import torch.nn as nn
import copy

from time import time

class Focus(nn.Module):
    """Focus width and height information into channel space."""

    def __init__(self, in_channels, out_channels, ksize=3, stride=1, deploy = False):
        super().__init__()
        self.deploy = deploy
        if self.deploy:
            pad = (ksize*2 - 1) // 2
            self.deploy_conv = nn.Conv2d(in_channels, out_channels, ksize*2, stride*2, pad)
        else:
            pad = (ksize - 1) // 2
            self.base_conv = nn.Conv2d(in_channels * 4, out_channels, ksize, stride, pad)
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = nn.SiLU(inplace=True)

    def weight_D2S(self, branch, shape, in_channels):
        kernel = branch.weight
        zero = torch.zeros(shape)
        zero[  :,  :, ::2,   ::2] = kernel[  :,:in_channels, ...]
        zero[  :,  :, ::2,  1::2] = kernel[  :,in_channels:in_channels*2, ...]
        zero[  :,  :,1::2,   ::2] = kernel[  :,in_channels*2:in_channels*3, ...]
        zero[  :,  :,1::2,  1::2] = kernel[  :,in_channels*3:, ...]
        return zero

    def switch_to_deploy(self):
        kernel_size = self.base_conv.kernel_size[0]*2
        shape = (self.base_conv.out_channels, self.base_conv.in_channels//4, kernel_size, kernel_size)
        deploy_k = self.weight_D2S(self.base_conv, shape, self.base_conv.in_channels//4)
        self.deploy_conv = nn.Conv2d(self.base_conv.in_channels//4, self.base_conv.out_channels, kernel_size, 2, (kernel_size - 1) // 2)
        self.deploy_conv.weight.data = deploy_k
        self.__delattr__('base_conv')
        self.deploy = True

    def forward(self, x):
        if self.deploy:
            return self.act(self.bn(self.deploy_conv(x)))
        else:
            patch_top_left = x[..., ::2, ::2]
            patch_top_right = x[..., ::2, 1::2]
            patch_bot_left = x[..., 1::2, ::2]
            patch_bot_right = x[..., 1::2, 1::2]
            x = torch.cat(
                (
                    patch_top_left,                    
                    patch_top_right,
                    patch_bot_left,
                    patch_bot_right,
                ),
                dim=1,
            )
            return self.act(self.bn(self.base_conv(x)))


def weights_init(net, init_type='normal', init_gain=0.02):
    def init_func(m):
        classname = m.__class__.__name__
        if hasattr(m, 'weight') and classname.find('Conv') != -1:
            if init_type == 'normal':
                torch.nn.init.normal_(m.weight.data, 0.0, init_gain)
            elif init_type == 'xavier':
                torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain)
            elif init_type == 'kaiming':
                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            elif init_type == 'orthogonal':
                torch.nn.init.orthogonal_(m.weight.data, gain=init_gain)
            else:
                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)
        elif classname.find('BatchNorm2d') != -1:
            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
            torch.nn.init.constant_(m.bias.data, 0.0)
    print('initialize network with %s type' % init_type)
    net.apply(init_func)

if __name__ == '__main__':
    N = 1
    C = 64
    H = 256
    W = 256
    O = 8
    groups = 4
 
    x = torch.randn(N, C, H, W)

    model = Focus(C, C*2, deploy=False)
    weights_init(model)
    time_start = time()
    for i in range(100):
        out = model(x)
    print('train forward time is', time() - time_start)

    model.switch_to_deploy()

    time_start = time()
    for i in range(100):
        deployout = model(x)
    print('deploy forward time is', time() - time_start)

    print('difference between the outputs of the training-time and converted ACB is')
    print(((deployout - out) ** 2).sum())
 

 
def rep_model_convert(model:torch.nn.Module, save_path=None, do_copy=True):
    if do_copy:
        model = copy.deepcopy(model)
    for module in model.modules():
        if hasattr(module, 'switch_to_deploy'):
            module.switch_to_deploy()
    if save_path is not None:
        torch.save(model.state_dict(), save_path)
    return model
